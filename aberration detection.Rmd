---
title: "Aberration detection methods using the surveillance package"
output:
  html_document:
    toc: yes
  word_document:
    toc: yes
---

```{r setup2, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
#install.packages(c('surveillance','zoo'))
library(surveillance)
library(zoo)
library(shiny)
library(lubridate)
library(ggplot2)
source('./R/App_Hist_limit.R')
source('./R/FarringtonApp.R')
source('./R/glrpoisApp.R')
source('./R/surv.ds.convert.R')


```


## What is an aberration? 

Let's first generate 5 years of weekly case data, where there is an average of 10 cases per week. We will do this be taking random samples from a Poisson distribution that has a mean of 5 cases. we can then plot the time series

When performing random number generation we use a 'seed' to ensure we can reproduce the results

```{r rand1}
set.seed(123) #set a seed so that random number generation is reproducible
```

Generate a time series of cases, with 5 years of weekly data (5*52 time points). The average number of cases is 5

```{r}

n.times <- 10*52 #5 weeks of weekly data

cases<- rpois( lambda=5, #Mean N cases 
               n=n.times    #How many observations (5 years of weekly data)
               )

date <- seq.Date(from=as.Date('2005-01-01'), length.out=n.times, by='week')

sim.ds <- cbind.data.frame('date'=date, cases)

head(sim.ds)
```

Plot the time series of cases 

```{r}

p1 <- ggplot(sim.ds , aes(x=date, y=cases)) +
  geom_line() +
  theme_classic() 

p1 + 
  geom_hline(yintercept=5, col='gray', lty=2)
```

We can see that there are some weeks when cases are above the average, and some cases below the average. And it even looks (by eye) like there might be some stretches where severla weeks in a row are above average. These number are *randomly generated*: the spikes in cases are real, but there is actually no shift in the underlying 'dynamics' of the system. There might be nothing out the ordinary to investigate (though some of these spikes might be due to actual clusters). We would want to have a way to say whether a particular week or series of weeks is *abnormal*. In other words, has there been a shift in the underlying dynamics?

Also simulate a time series with seasonality and random noise.
```{r}
n.times <- 10*52 #5 weeks of weekly data

log.lambda <- log(5) + 0.5*sin(2*pi*(1:n.times)/52) 

cases.seas<- rpois( lambda=exp(log.lambda), #Mean N cases 
               n=n.times    #How many observations (5 years of weekly data)
               )

date <- seq.Date(from=as.Date('2005-01-01'), length.out=n.times, by='week')
sim.ds.seas <- cbind.data.frame('date'=date, cases.seas)

head(sim.ds.seas)
```

```{r}
p2 <- ggplot(sim.ds.seas , aes(x=date, y=cases.seas)) +
  geom_line() +
  theme_classic() 
p2 +
    geom_hline(yintercept=5, col='gray', lty=2)

```


### Setting a threshold
The simplest way to set a threshold in an uncomplicated situation like this would be to do something like flag weeks where the number of cases is a certain amount above average. This is sometimes done by calculating the standard deviation of the observed cases and setting a multiple of this as our threshold. in our example, we can see that several weeks are above the threshold during the 5 years of observation. Depending on what disease we are monitoring, this might be too sensitive (as none of these are real aberrations), or not sensitive enough.

What happens if we shift the threshold to be 3 SD above the mean?

```{r rand2 }
mean.cases<-mean(cases) #Calculate the mean number of cases
sd.cases<-sd(cases) # Calculate standard deviation of the cases
```

Define a threshold

```{r}
threshold= mean.cases + 2*sd.cases #Set sd at mean + 2*SD
```

#Plot estimates

```{r}
p1 + 
  geom_hline(yintercept=threshold, col='red', lty=2)
```

```{r}
threshold.seas <- mean(cases.seas) + 2*sd(cases.seas)
p2 + 
  geom_hline(yintercept=threshold.seas, col='red', lty=2)
```

### Why this is an oversimplification

Often the data are more complicated than this. We should be using the correct distribution for rare count data (Poisson or negative binomial) when estimating the threshold. We also might need to adjust for seasonality, or trends in the data. And we might want to detect if several weeks in a row are higher than typical. That is where the algorithms in the surveillance package come in handy.


## Introduction to the Surveillance package

The surveillance package in R has a number of commonly-used aberration detection algorithms. These include some very simple algorithms (historical limits method--ie algo.cdc), as well as some highly sophisticated tools (hidden markov models, geospatial models)
We will go through the analysis of some data on *Salmonella agona* from the UK (1990-1995) that is included with the package. We will also go through some examples laid out in Salmon et al., JSS



Import the Salmonella case data provided with the surveillance package
```{r}
# data("salmonella.agona")
# time<-seq.Date(from=as.Date("1990-01-01"),length.out=length(salmonella.agona$observed), by='week')
# ds1<-cbind.data.frame('date'=time, 'cases'=salmonella.agona$observed)
# head(ds1)
# saveRDS(ds1,'./Data/sal1.rds')

ds1 <- readRDS('./Data/sal1.rds')
```

view first 10 rows
```{r ds.explore,echo=TRUE}
head(ds1)
```

Plot the salmonella data
```{r}
p1 <- ggplot(ds1 , aes(x=date, y=cases)) +
  geom_line() +
  theme_classic() 
p1
```


### Historical limits method

This is a simple method used in some CDC reports of routinely-reported diseases. The method was first described by Stroup et al (Statistics in Medicine 1989). It takes the value in the current week, and adds it to data from the previous 3 weeks to create a 4 week period. The algorithm then compares this against the corresponding 4 week period from the previous 5 years.

historical period occurring at the same time of year. So if we are interested in whether February 2019 counts of measles are unusual, we would take the average of the values of January, February, and March 2014-2018. This gives 5 years*3 months=15 historical data points. We take the mean and variance of these values to create a threshold. Note it aggregates the weekly data into 4 week 'months', and uses these 4 week blocks as the basis for analysis.

On the positive side, this is a simple, intuitive way to calculate a threshold. And it inherently adjusts for seasonality (because we are only comparing to the same time of year in previous seasons). On the downside, we need at least 5 years of historical data. And there is no way to adjust for trends in the data or to downweight past epidemics.


m: how many time points on either side to use as reference (typical is 1)
b: number of years to go back in time
m=1, b=5 will give 15 reference data points

Let's try first with our simulated dataset

```{r hist.limits2}
app.hist.limit(ds=sim.ds.seas, datevar='date', casevar='cases.seas')
```

Then try with our real data

```{r hist.limits2}
app.hist.limit(ds=ds1, datevar='date', casevar='cases')
```



## Farrington method

Many public health agencies use variations of an algorithm developed by Farrington, where we are testing whether the observed number of cases at a particular time point are above an epidemic threshold. 

This method has several advantages:
1. Tests for and adjusts for trend automatically
2. Iterative process that downweights influence of past epidemics (increasing chances of detecting future epidemics)
3. Like the Historical limits method, this method deals with seasonality by only taking values from the same time of year when setting a threshold.
4. Designed for count data and doesn't make assumptions about the data being normally distributed; this is more appropriate for sparse data

*What happens if you don't down-weight past epidemics?*

The gray lines here show you the observed number of cases at each week. The black dots show which data are used for model fitting. The purple dash shows the epidemic threshold for the current week.

Let's try first with our simulated dataset

```{r hist.limits2}
FarringtonApp(ds=sim.ds.seas, datevar='date', casevar='cases.seas')
```

```{r farrington1, echo=TRUE}
FarringtonApp(ds=ds1, datevar='date', casevar='cases')
```



## CUSUM approaches

All of the methods discussed so far evaluate whether the number of cases at a specific time point exceed an epidemic threshold. However, we often interested in seeing if there has been a change in the underlying risk the shows up in multiple consecutive time points. Methods that evaluate the CUmulative SUM (CUSUM) methods are designd to do this and are often more robust and ensitivte to accumulated changes.


There are many different CUSUM-type algorithms. We will use one called glrnb. This allows for seasonal and trend adjustment. It works well with count data.

If you specify season=T or trend=T, it fits a trend to data before the evaluation period. Here we hold the first year of data out to allow for training of the seasonality component

### Here is an alternative algorithm for seasonal CUSUM 

First on the simulated data...
```{r}
glrpois_App(ds=sim.ds.seas, datevar='date', casevar='cases.seas')

```

Now try with real data. 
-What happens if you turn trend to true?
```{r glr.seas,fig.width=6, fig.height=8}

glrpois_App(ds=ds1, datevar='date', casevar='cases')

```

## Simulated data where there is a known change

```{r}
set.seed(123)
n.times <- 10*52 #5 weeks of weekly data

#rate increases 30% at weeks 450-454
rr <- rep(1, times=n.times)
rr[450:456] <- 2
  

log.lambda2 <- log(5) + 0.5*sin(2*pi*(1:n.times)/52) + log(rr)

plot(log.lambda2, type='l')

cases.seas2<- rpois( lambda=exp(log.lambda2), #Mean N cases 
               n=n.times    #How many observations (5 years of weekly data)
               )

date <- seq.Date(from=as.Date('2005-01-01'), length.out=n.times, by='week')
sim.ds.seas2 <- cbind.data.frame('date'=date, cases.seas2)

head(sim.ds.seas2)

plot(cases.seas2)
```

```{r}
app.hist.limit(ds=sim.ds.seas2, datevar='date', casevar='cases.seas2')

```

```{r}
FarringtonApp(ds=sim.ds.seas2, datevar='date', casevar='cases.seas2')

```

```{r}
glrpois_App(ds=sim.ds.seas2, datevar='date', casevar='cases.seas2')

```



## Getting into the weeds of coding

The analyses above use some wrapper functions to make things easier. If you wanted to do this yourself without the shiny app, you would do the following:

### Step 1: Convert your data to a format the surveillance package can recognize
```{r}
year.start <- min(year(ds1[,'date']))
  week.start <- min(week(ds1[,'date'])[year(ds1[,'date'])==year.start])
  
  SurvObj1 <- create.disProg(
    week = 1:nrow(ds1), #Index of observations
    observed = ds1[,'cases'] ,  #cases
    state=matrix(0, nrow=nrow(ds1), ncol=1), #just 0s
    start = c(year.start, week.start)) #start date; 1st week of 1990
  
  str(SurvObj1)
```

### Step 2: Call the algorithm of interest

```{r}
week.test <- 270
mod1<-algo.farrington(SurvObj1, #dataset to use
                            control=list(range=c(week.test), #Which week(s) do you want to test for aberrations
                                         b=5, #How many years of historical data to use
                                         w=3, #Number of weeks before and after current week to include in                                 model fitting
                                         reweight=TRUE, #Do you want to downweight past epidemics?
                                         plot=FALSE,
                                         alpha=0.05
                            ))
```

### Step 3: Plot the output
```{r}
 col.vec<-rep(1, times=length(observed))
      col.vec[c(week.test) ]<-2 + mod1$alarm*2
      #col.vec[train.points]<-3
      col.select<-c('white', 'blue', 'black','red')
      
      cols<-c('gray', 'black', 'red')
      plot.obs <- mod1$disProgObj$observed
      plot.obs[week.test+1 : length(plot.obs)] <- NA
      plot(plot.obs , pch=16 , bty='l', ylab='Cases', xlab='Week',  col=col.select[col.vec])
      points(c(rep(NA, times=(week.test-1)) , mod1$upperbound), type='p', col='purple', pch="-")
      points(plot.obs ,type='l', col='gray')
      
      title('Farrington. Cases vs threshold: alarms are RED')
```

