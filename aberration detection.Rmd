---
title: "Aberration detection methods using the surveillance package"
output:
  html_document:
    toc: yes
  word_document:
    toc: yes
---

```{r setup2, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
#install.packages(c('surveillance','zoo'))
library(surveillance)
library(zoo)
library(shiny)
library(lubridate)
library(ggplot2)
source('./R/App_Hist_limit.R')
```


## What is an aberration? 

Let's first generate 5 years of weekly case data, where there is an average of 10 cases per week. We will do this be taking random samples from a Poisson distribution that has a mean of 5 cases. we can then plot the time series

When performing random number generation we use a 'seed' to ensure we can reproduce the results

```{r rand1}
set.seed(123) #set a seed so that random number generation is reproducible
```

Generate a time series of cases, with 5 years of weekly data (5*52 time points). The average number of cases is 5

```{r}

n.times <- 5*52 #5 weeks of weekly data

cases<- rpois( lambda=5, #Mean N cases 
               n=n.times    #How many observations (5 years of weekly data)
               )

sim.ds <- cbind.data.frame('t'=1:n.times, cases)

head(sim.ds)
```

Plot the time series of cases 

```{r}

p1 <- ggplot(sim.ds , aes(x=t, y=cases)) +
  geom_line() +
  theme_classic() 

p1 + 
  geom_hline(yintercept=5, col='gray', lty=2)
```

We can see that there are some weeks when cases are above the average, and some cases below the average. And it even looks (by eye) like there might be some stretches where severla weeks in a row are above average. These number are *randomly generated*: the spikes in cases are real, but there is actually no shift in the underlying 'dynamics' of the system. There might be nothing out the ordinary to investigate (though some of these spikes might be due to actual clusters). We would want to have a way to say whether a particular week or series of weeks is *abnormal*. In other words, has there been a shift in the underlying dynamics?

### Setting a threshold
The simplest way to set a threshold in an uncomplicated situation like this would be to do something like flag weeks where the number of cases is a certain amount above average. This is sometimes done by calculating the standard deviation of the observed cases and setting a multiple of this as our threshold. in our example, we can see that several weeks are above the threshold during the 5 years of observation. Depending on what disease we are monitoring, this might be too sensitive (as none of these are real aberrations), or not sensitive enough.

What happens if we shift the threshold to be 3 SD above the mean?

```{r rand2 }
mean.cases<-mean(cases) #Calculate the mean number of cases
sd.cases<-sd(cases) # Calculate standard deviation of the cases
```

Define a threshold

```{r}
threshold= mean.cases + 2*sd.cases #Set sd at mean + 2*SD
```

#Plot estimates

```{r}
p1 + 
  geom_hline(yintercept=threshold, col='red', lty=2)
```

### Why this is an oversimplification

Often the data are more complicated than this. We should be using the correct distribution for rare count data (Poisson or negative binomial) when estimating the threshold. We also might need to adjust for seasonality, or trends in the data. And we might want to detect if several weeks in a row are higher than typical. That is where the algorithms in the surveillance package come in handy.


## Introduction to the Surveillance package

The surveillance package in R has a number of commonly-used aberration detection algorithms. These include some very simple algorithms (historical limits method--ie algo.cdc), as well as some highly sophisticated tools (hidden markov models, geospatial models)
We will go through the analysis of some data on *Salmonella agona* from the UK (1990-1995) that is included with the package. We will also go through some examples laid out in Salmon et al., JSS

## Setting up your data to use with the surveillance package
The surveillance package requires that the data be formatted in a specific way. We have a data frame that has 312 rows (for weeks), and 2 columns: date of the start of the week, and case count. Looking at the first 10 observations, we can see the data start week 1 of 1990

Import the data provided with the surveillance package
```{r}
# data("salmonella.agona")
# time<-seq.Date(from=as.Date("1990-01-01"),length.out=length(salmonella.agona$observed), by='week')
# ds1<-cbind.data.frame('date'=time, 'cases'=salmonella.agona$observed)
# head(ds1)
# saveRDS(ds1,'./Data/sal1.rds')

ds1 <- readRDS('./Data/sal1.rds')
```

view first 10 rows
```{r ds.explore,echo=TRUE}
head(ds1)
```


```{r}
p1 <- ggplot(ds1 , aes(x=date, y=cases)) +
  geom_line() +
  theme_classic() 
p1
```


### Historical limits method

This is a simple method used in some CDC reports of routinely-reported diseases. The method was first described by Stroup et al (Statistics in Medicine 1989). It takes the value in the current week, and adds it to data from the previous 3 weeks to create a 4 week period. The algorithm then compares this against the corresponding 4 week period from the previous 5 years.

historical period occurring at the same time of year. So if we are interested in whether February 2019 counts of measles are unusual, we would take the average of the values of January, February, and March 2014-2018. This gives 5 years*3 months=15 historical data points. We take the mean and variance of these values to create a threshold. Note it aggregates the weekly data into 4 week 'months', and uses these 4 week blocks as the basis for analysis.

On the positive side, this is a simple, intuitive way to calculate a threshold. And it inherently adjusts for seasonality (because we are only comparing to the same time of year in previous seasons). On the downside, we need at least 5 years of historical data. And there is no way to adjust for trends in the data or to downweight past epidemics.


m: how many time points on either side to use as reference (typical is 1)
b: number of years to go back in time
m=1, b=5 will give 15 reference data points

```{r hist.limits2}
app.hist.limit(ds=ds1, datevar='date', casevar='cases')
```



## Farrington method

Many public health agencies use variations of an algorithm developed by Farrington, where we are testing whether the observed number of cases at a particular time point are above an epidemic threshold. 

This method has several advantages:
1. Tests for and adjusts for trend automatically
2. Iterative process that downweights influence of past epidemics (increasing chances of detecting future epidemics)
3. Like the Historical limits method, this method deals with seasonality by only taking values from the same time of year when setting a threshold.
4. Designed for count data and doesn't make assumptions about the data being normally distributed; this is more appropriate for sparse data

*What happens if you don't downweight past epidemics?*

```{r farrington1, echo=TRUE}
#for(i in c(260:270))


shinyApp(
  ui=fluidPage(
    

    sliderInput("week.test", "Current Week:",
                min=273, max=312, value=273),
      selectInput("set.alpha", "Alpha",
                choices=list(0.01,0.025, 0.05, 0.1), selected=0.025),
    selectInput("set.b", "Number of historical years",
                choices=list(2,3,4,5,6), selected=c(5)),
     selectInput("set.m", "Number of surrounding 4-week months to use",
                choices=list(1,2), selected=c(1)),
    plotOutput("periodPlot")
  ),
  server=function(input, output){
     output$periodPlot = renderPlot({
       
b=input$set.b
m=input$set.m
freq=52

mod1<-algo.farrington(surv.ds, #dataset to use
                control=list(range=c(input$week.test),
                b=5, #How many years of historical data to use
                w=3, #Number of weeks before and after current week to include in                                 model fitting
                reweight=TRUE, #Do you want to downweight past epidemics?
                plot=FALSE,
                alpha=input$set.alpha
                ))

col.alarms<-c(rep(1, times=(mod1$control$range[1])-1) , (mod1$alarm+2))
cols<-c('gray', 'black', 'red')
plot(mod1$disProgObj$observed , pch=16 , bty='l', ylab='Cases', xlab='Week', col=cols[col.alarms])
points(c(rep(NA, times=(mod1$control$range[1]-1)) , mod1$upperbound), type='l')
title('Farrington. Cases vs threshold: alarms are RED')

    },width = "auto", height = "auto")
  }
)
```



## CUSUM approaches

All of the methods discussed so far evaluate whether the number of cases at a specific time point exceed an epidemic threshold. However, we often interested in seeing if there has been a change in the underlying risk the shows up in multiple consescutive time points. Methods that evaluate the CUmulative SUM (CUSUM) methods are designd to do this and are often more robust and ensitivte to accumulated changes.

##Play around by directly modifying K and H. 
*Run this chunk.*
```{r ,echo=FALSE }
shinyApp(
  ui=fluidPage(
    

    sliderInput("K.set", "K threshold:",
                min=0, max=max(salmonellaDisProg$observed[,1]), value=2),
    sliderInput("H.set", "H threshold:",
               min=1, max=100, value=2),
    plotOutput("periodPlot")
  ),
  server=function(input, output){
     output$periodPlot = renderPlot({
     in.control.mean<- mean(salmonellaDisProg$observed[-c(1:52)])
      epidemic.increase=input$epidemic.increase #how big of an increase do you want to detect?
      
      s1=(epidemic.increase-1)/sqrt(in.control.mean)
      sd.y<-sd(salmonellaDisProg$observed[,1])
      mean.y<-mean(salmonellaDisProg$observed[,1])

      
      cusum1<-algo.cusum(salmonellaDisProg, control = list(range=53:312, k = input$K.set , h = input$H.set, trans = "standard", alpha = NULL ))
      
      K.std<- input$K.set*sd.y +mean.y
      par(mfrow=c(2,1))
      plot(cusum1$disProgObj$observed[53:312], bty='l',type='l', ylab='Cases')
      abline(h= K.std)
      title(paste0('Observed data with K=',round(input$K.set,2)))
      
      plot(cusum1$cusum, type='p', bty='l',pch=16, col=cusum1$alarm+1, ylab='CUSUM')
      abline(h= input$H.set)
      title(paste0('CUSUM statistic with H=', round(input$H.set,2),' red=ALARM'))

      
    },width = "auto", height = "auto")
  }
)
```

Automated h level detection (doesn't always work super well)
```{r ,echo=FALSE }
shinyApp(
  ui=fluidPage(
    

    sliderInput("ARL", "ARL:",
                min=13, max=520, value=104),
    sliderInput("SD.increase", "SD Increase:",
               min=0.25, max=5, value=2),
    plotOutput("periodPlot")
  ),
  server=function(input, output){
     output$periodPlot = renderPlot({
      sd.y<-sd(salmonellaDisProg$observed[,1])
      mean.y<-mean(salmonellaDisProg$observed[,1])
      
       in.control.mean<- mean(salmonellaDisProg$observed[-c(1:52)])
      ARL.set <- input$ARL #an ARL of 104 says we want 1 false alarm ever 2 years (2*52 weeks)
 
      #s1=(epidemic.increase-1)/sqrt(in.control.mean)
      s1=(input$SD.increase-mean.y)/sd.y
      optimized.parms<-findH(ARL0=ARL.set, theta0=in.control.mean, s = s1, rel.tol = 0.1, roundK = FALSE, distr = c("poisson"))

      # theta1 <- in.control.mean*epidemic.increase
      # s1=(theta1-in.control.mean)/sqrt(in.control.mean)
      # optimized.parms<-findH(ARL0=ARL.set, theta0=in.control.mean, s = 1, rel.tol = 0.03, roundK = FALSE, distr = c("poisson"))

      k.optimized<- optimized.parms['k']
      h.optimized<- optimized.parms['h']
      K.std<- k.optimized*sd.y +mean.y

      cusum1<-algo.cusum(salmonellaDisProg, control = list(range=53:312, k = k.optimized , h = h.optimized, 
                                                           trans = "standard", alpha = NULL ))
      
      par(mfrow=c(2,1))
      plot(cusum1$disProgObj$observed[53:312], bty='l',type='l', ylab='Cases')
      abline(h= K.std )
      title(paste0('Observed data with K=',round(k.optimized,2), ', ARL=', ARL.set, ' weeks'))
      
      plot(cusum1$cusum, type='p', bty='l',pch=16, col=cusum1$alarm+1, ylab='CUSUM')
      abline(h= h.optimized)
      title(paste0('CUSUM statistic with H=', round(h.optimized,2),' red=ALARM'))

      
    },width = "auto", height = "auto")
  }
)
``` 

#### Unadjusted Poisson CUSUM 

Let's say we want to detect a 2-fold increase in the mean number of cases compared to the 'in-control' mean. We will empircally determine the in-control mean by looking at the average number of cases in the first 52 weeks. And we will set an arbitrary threshold of 4 for the H value.

```{r cusum-simple2, fig.width=6, fig.height=8}
in.control.mean<- mean(salmonellaDisProg$observed[1:52])
epidemic.increase=2 #how big of an increase do you want to detect?
epidemic.obs<-in.control.mean*epidemic.increase
h.threshold=4 #What is H threshold?
 sd.y<-sd(salmonellaDisProg$observed[,1])
mean.y<-mean(salmonellaDisProg$observed[,1])
k.select<-findK( theta0=in.control.mean ,theta1=in.control.mean*epidemic.increase, dist='poisson')

cusum1<-algo.cusum(salmonellaDisProg, control = list(range=53:312, k = k.select, h = h.threshold, trans = "standard", alpha = NULL ))
K.std<- k.select*sd.y +mean.y

par(mfrow=c(2,1), mar=c(2,3,1,1))
plot(cusum1$disProgObj$observed[53:312], bty='l',type='l')
abline(h= K.std)
title('Observed data with K threshold')

plot(cusum1$cusum, type='p', bty='l',pch=16, col=cusum1$alarm+1)
abline(h= cusum1$control$h)
title('CUSUM statistic with H threshold; red=ALARM')
```

## How do we optimize the K and H thresholds?

The surveillance package has a function called arlCusum to help with this. We use the concept of "Average Run Length", which is how long we many time periods we want to have between false positive signals. If we have a short ARL, we will have more false signals (but more sensitivity), a longer ARL will give fewer false signals (but lower sensitivity). You can play around with the H threshold and the 'epidemic.increase' parameter here to see how it affects the estimates. See what happens when you play around with the ARL

```{r cusum-arl,  fig.width=6, fig.height=8}

in.control.mean<- mean(salmonellaDisProg$observed[1:52])
sd.y<-sd(salmonellaDisProg$observed[,1])
mean.y<-mean(salmonellaDisProg$observed[,1])
ARL.set <- 104 #an ARL of 104 says we want 1 false alarm ever 2 years (2*52 weeks)
epidemic.increase=2 #how big of an increase do you want to detect?

theta1 <- in.control.mean*epidemic.increase
s1=(theta1-in.control.mean)/sqrt(in.control.mean)

optimized.parms<-findH(ARL0=ARL.set, theta0=in.control.mean, s = s1, rel.tol = 0.03, roundK = FALSE, distr = c("poisson"))

k.optimized<- optimized.parms['k']
h.optimized<- optimized.parms['h']
K.std<- k.optimized*sd.y +mean.y

cusum1<-algo.cusum(salmonellaDisProg, control = list(range=53:312, k = k.optimized , h = h.optimized, trans = "standard", alpha = NULL ))

par(mfrow=c(2,1), mar=c(2,2,1,1))
plot(cusum1$disProgObj$observed[53:312], bty='l',type='l')
abline(h= K.std )
title(paste0('Observed data with K=',round(k.optimized,2), ', ARL=', ARL.set, ' weeks'))

plot(cusum1$cusum, type='p', bty='l',pch=16, col=cusum1$alarm+1)
abline(h= h.optimized)
title(paste0('CUSUM statistic with H=', round(h.optimized,2),' red=ALARM'))
```



 

## What if there is seasonality in the data?
-Modified CUSUM-type approaches exist (algo.rogerson, algo.glrpois)
-Or we can use regression-based approaches to detect increase in a single time point
-Next time we will discuss the fitting and use of these harmonic models to account for both seasonality and trend


###  use the CUSUM algorithm with seasonal adjustment
What happens when you play around with K threshold here? (ie try to crank it up to 4)
```{r cusum.seas, fig.width=6, fig.height=8}
k.set=5
h.set=2
cusum1<-algo.cusum(salmonellaDisProg, control = list(range=53:312, 
                                                k = k.set , 
                                                h = h.set, 
                                                m='glm', #turn on seasonal adjustment
                                                trans = "standard", 
                                                alpha = NULL ))

m.plot<-c(rep(NA, times=(cusum1$control$range[1]-1)),cusum1$control$m)
cusum.plot<-c(rep(NA, times=(cusum1$control$range[1]-1)),cusum1$cusum)
alarm.vec<- c(rep(1, times=(cusum1$control$range[1]-1)),cusum1$alarm+2 )
col.alarm.vec=c('gray', 'black','red')
sd.y<-sd(salmonellaDisProg$observed[,1])
mean.y<-mean(salmonellaDisProg$observed[,1])
#K.std<- ((k.set-mean.y)/sd.y )+ m.plot
K.std<- k.set+ m.plot

par(mfrow=c(2,1))
plot(cusum1$disProgObj$observed, bty='l',type='p', ylab='Observed cases',col=col.alarm.vec[alarm.vec], pch=16)
#abline(h= k.optimized )
points(m.plot, type='l', col='gray', lty=2)
points(K.std, type='l', col='gray', lty=2)
title(paste0('Observed data',k.set))

plot(cusum.plot, type='p', bty='l',pch=16, col=col.alarm.vec[alarm.vec])
abline(h= h.set)
title(paste0('CUSUM statistic with H=', h.set,' red=ALARM'))
```

### Here is an alternative algorithm for seasonal CUSUM 
-What happens if you turn trend to true?
```{r glr.seas,fig.width=6, fig.height=8}
mod1<- algo.glrpois(salmonellaDisProg,control=list(
   range=c(53:312),
   c.ARL=5,
   M=-1, #How many time points back should we look? Negative 1: use all cases
   ret=c('value'),
  mu0=list( trend=F, #Trend adjustment?
   S=1) #Seasonality? 0=no, 1 or 2 = # harmonics to include
))

glr.vec<- c(rep(NA, times=(mod1$control$range[1]-1)),mod1$upperbound[,1] )
m.vec<- c(rep(NA, times=(mod1$control$range[1]-1)),mod1$control$mu0 )

alarm.vec<- c(rep(1, times=(mod1$control$range[1]-1)),mod1$alarm+2 )
col.alarm.vec=c('gray', 'black','red')

par(mfrow=c(2,1), mar=c(2,2,2,1))
plot(mod1$disProgObj$observed[,1], bty='l',type='p', ylab='Observed cases',col=col.alarm.vec[alarm.vec], pch=16)
points(m.vec, type='l', col='black')
title('Observed cases and mean')

plot(glr.vec, bty='l',type='p', ylab='GLR statistic',col=col.alarm.vec[alarm.vec], pch=16)
abline(h=mod1$control$c.ARL, lty=2)
title('GLR statistic')
#abline(h= k.optimized )
#title(paste0('Observed data with K=',round(k.optimized,2), ', ARL=', ARL.set, ' weeks'))


```

EARS designed to work with daily data; rapid signal
```{r ears,fig.width=6, fig.height=8}
# 
# stsObj <- disProg2sts( salmonellaDisProg)
# ear.mod<-earsC(stsObj,control = list(range=53:312, 
#                                                method='C3' ))
# par(mfrow=c(1,1))
# plot(ear.mod)

```
